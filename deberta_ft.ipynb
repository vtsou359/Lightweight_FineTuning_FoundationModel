{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine Tuning DeBerta",
   "id": "ab1b51ff62410a34"
  },
  {
   "cell_type": "code",
   "id": "3afc9c71868e81e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T21:51:45.098254Z",
     "start_time": "2025-01-22T21:51:22.727101Z"
    }
   },
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "mdl_tok_name = \"microsoft/deberta-base\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T22:00:58.380147Z",
     "start_time": "2025-01-22T22:00:58.374621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing required function\n",
    "from torch.cuda import memory_summary, is_available\n",
    "\n",
    "# Check if CUDA is available\n",
    "if is_available():\n",
    "\tprint(memory_summary(device = None, abbreviated = False))\n",
    "else:\n",
    "\tprint(\"CUDA is not available on this system. Please ensure that a CUDA-capable device is properly configured.\")\n"
   ],
   "id": "8c32abd1e37b06f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available on this system. Please ensure that a CUDA-capable device is properly configured.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T22:02:01.054620Z",
     "start_time": "2025-01-22T22:02:00.969766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())  # Number of CUDA-capable devices\n",
    "#print(torch.cuda.get_device_name(0))  # Name of the first CUDA device\n"
   ],
   "id": "7141a5fa5f3c422c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available())\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice_count())  \u001B[38;5;66;03m# Number of CUDA-capable devices\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_device_name\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# Name of the first CUDA device\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:493\u001B[0m, in \u001B[0;36mget_device_name\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    481\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_device_name\u001B[39m(device: Optional[_device_t] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    482\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Get the name of a device.\u001B[39;00m\n\u001B[0;32m    483\u001B[0m \n\u001B[0;32m    484\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    491\u001B[0m \u001B[38;5;124;03m        str: the name of the device\u001B[39;00m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 493\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_device_properties\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mname\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:523\u001B[0m, in \u001B[0;36mget_device_properties\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    513\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_device_properties\u001B[39m(device: _device_t) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _CudaDeviceProperties:\n\u001B[0;32m    514\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Get the properties of a device.\u001B[39;00m\n\u001B[0;32m    515\u001B[0m \n\u001B[0;32m    516\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    521\u001B[0m \u001B[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001B[39;00m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 523\u001B[0m     \u001B[43m_lazy_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# will define _get_device_properties\u001B[39;00m\n\u001B[0;32m    524\u001B[0m     device \u001B[38;5;241m=\u001B[39m _get_device_index(device, optional\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m device \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m device_count():\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[0;32m    309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    313\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    314\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Lora Configuration for DeBerta model",
   "id": "c097bfd4ea37c7b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:30.499675Z",
     "start_time": "2025-01-21T21:43:30.493879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config = LoraConfig(\n",
    "\tr = 4,  # Low-rank dimension: 4 or 2\n",
    "\ttarget_modules = [\n",
    "\t\t#\"attention.self.in_proj\",\n",
    "\t\t#\"attention.output.dense\",\n",
    "\t\t#\"intermediate.dense\",\n",
    "\t\t\"output.dense\"\n",
    "\t],\n",
    "\ttask_type = TaskType.SEQ_CLS,  # Sequence Classification task\n",
    "\tlora_alpha = 4,  # Scaling factor: use 8 and if memory is still an issue use 4\n",
    "\tlora_dropout = 0.2  # Dropout rate 0.2 for regularisation or 0.1 to stabilize training efficiency\n",
    ")"
   ],
   "id": "7ab2a33cef283a6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading the filtered dataset",
   "id": "d044bb1571cbc875"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:37.026127Z",
     "start_time": "2025-01-21T21:43:36.317301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the file path to the dataset\n",
    "file_path = Path(\"data/filtered_dataset.csv\")\n",
    "\n",
    "# Load the dataset using Hugging Face's `load_dataset`\n",
    "dataset = load_dataset('csv', data_files = str(file_path))\n",
    "\n",
    "# Inspect the unique values in the 'labels' column\n",
    "product_classes = dataset[\"train\"].unique(\"Product\")\n",
    "\n",
    "# Convert the 'Product' column to a ClassLabel feature\n",
    "product_label = ClassLabel(names=product_classes)\n",
    "dataset = dataset.cast_column(\"Product\", product_label)\n",
    "\n",
    "# Rename the columns: \"Product\" to \"labels\", and \"Consumer complaint narrative\" to \"complaint\"\n",
    "dataset = dataset.rename_column(\"Product\", \"labels\")\n",
    "dataset = dataset.rename_column(\"Consumer complaint narrative\", \"complaint\")\n",
    "\n",
    "# Extract the features (columns) we want\n",
    "dataset = \\\n",
    "    dataset[\"train\"].select_columns(\n",
    "        [\"complaint\", \"labels\"]\n",
    "    ).train_test_split(\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        seed=23,\n",
    "        stratify_by_column=\"labels\"\n",
    "    )\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the resulting dataset\n",
    "print(dataset)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['complaint', 'labels'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['complaint', 'labels'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Inspecting the labels\n",
    "\n",
    "Credit card is labeled as 0 and  Mortgage is labeled as 1"
   ],
   "id": "5515a2b1ef507acb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:39.524799Z",
     "start_time": "2025-01-21T21:43:39.514835Z"
    }
   },
   "cell_type": "code",
   "source": "product_label",
   "id": "b9040be5e710a56f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['Mortgage', 'Credit card or prepaid card'], id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Preprocess dataset\n",
    "\n",
    "Tokenizing 'Consumer complaint narrative' feature values"
   ],
   "id": "d33a3db87bf72169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:42.439971Z",
     "start_time": "2025-01-21T21:43:41.813117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(mdl_tok_name)\n",
    "\n",
    "# Let's use a lambda function to tokenize all the examples\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"complaint\"],\n",
    "                            truncation=True,\n",
    "                            padding=True,\n",
    "                            return_tensors = \"pt\"\n",
    "                            ),\n",
    "\t    batched=True,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ],
   "id": "4de95d07c818cebd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['complaint', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading Model",
   "id": "7edce09ded4bded4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:48.349316Z",
     "start_time": "2025-01-21T21:43:47.396039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    mdl_tok_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"Mortgage\", 1: \"Credit card or prepaid card\"},\n",
    "    label2id={\"Credit card or prepaid card\": 0, \"Mortgage\": 1},\n",
    ")\n",
    "print(model)"
   ],
   "id": "382a8fe8d3b4e8d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaForSequenceClassification(\n",
      "  (deberta): DebertaModel(\n",
      "    (embeddings): DebertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
      "      (LayerNorm): DebertaLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): DebertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaLayer(\n",
      "          (attention): DebertaAttention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
      "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): DebertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): DebertaLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): DebertaLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(1024, 768)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:51.125057Z",
     "start_time": "2025-01-21T21:43:51.121022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print all module names in the model to identify target modules\n",
    "#for name, module in model.named_modules():\n",
    "#\tprint(name, \":\", type(module))"
   ],
   "id": "a25fe247758321e1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:52.740636Z",
     "start_time": "2025-01-21T21:43:52.686831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "id": "92bbb4de048aa7a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 259,586 || all params: 139,453,444 || trainable%: 0.1861\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining Evaluation Metrics as a function",
   "id": "3c751fe02b91ed22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:43:59.562953Z",
     "start_time": "2025-01-21T21:43:59.557520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\t# Unpack predictions and labels\n",
    "\tpredictions, labels = eval_pred\n",
    "\t# Get the predicted class (argmax selects the class with the highest score)\n",
    "\tpredictions = np.argmax(predictions, axis = 1)\n",
    "\t# Compute metrics\n",
    "\taccuracy = accuracy_score(y_true = labels, y_pred = predictions)\n",
    "\tprecision = precision_score(y_true =labels, y_pred =predictions)\n",
    "\trecall = recall_score(y_true = labels, y_pred = predictions)\n",
    "\tf1 = f1_score(y_true = labels, y_pred = predictions)\n",
    "\t# Return all metrics\n",
    "\treturn {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ],
   "id": "a78ec1a63874f292",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Define Trainer to fine-tuning the foundation model\n",
    "\n",
    "The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "\n",
    "You can find more at this [link](https://huggingface.co/docs/transformers/main_classes/trainer)."
   ],
   "id": "fb730de6ffb08b74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T21:44:45.634577Z",
     "start_time": "2025-01-21T21:44:45.554067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "\tmodel = model,\n",
    "\targs = TrainingArguments(\n",
    "\t\toutput_dir = \"./data/creditc_mortg\",\n",
    "\t\tlearning_rate = 2e-5,  # 2e-5 is conservative and wee=ll-suited, if training speed is critical start higher: 5e-5\n",
    "\t\tper_device_train_batch_size = 2,  # Reduce batch size for memory stability\n",
    "\t\tper_device_eval_batch_size = 2,  # Same for evaluation\n",
    "\t\teval_strategy = \"steps\",  # Evaluate every few steps instead of epochs\n",
    "\t\tsave_strategy = \"steps\",  # Save checkpoint every few steps\n",
    "\t\teval_steps = 200,  # Evaluation frequency\n",
    "\t\tsave_steps = 200,  # Save checkpoint frequency\n",
    "\t\tlogging_steps = 200, # Log every 50 steps\n",
    "\t\tsave_total_limit = 2,  # Keep only most recent checkpoints\n",
    "\t\tnum_train_epochs = 2,  # Start with 1 epoch for testing, increase later\n",
    "\t\tweight_decay = 0.01,  # Standard for regularization\n",
    "\t\twarmup_steps = 50, # warmup steps for LR scheduling\n",
    "\t\tgradient_accumulation_steps = 2,  # Simulate larger effective batch size\n",
    "\t\tlogging_strategy = \"steps\",  # Reduce logging frequency\n",
    "\t\tfp16 = True,  # Enable mixed precision (if GPU available)\n",
    "\t\tdataloader_num_workers = 1,  # Limit data loading threads to 1. If slow, then increase to 2-4 (it may consume more CPU resources though)\n",
    "\t\tload_best_model_at_end = True,\n",
    "\t\tuse_cpu = False,  # Default to CPU; remove for GPU\n",
    "\t),\n",
    "\ttrain_dataset = tokenized_dataset[\"train\"],\n",
    "\teval_dataset = tokenized_dataset[\"test\"],\n",
    "\ttokenizer = tokenizer,\n",
    "\tdata_collator = DataCollatorWithPadding(tokenizer = tokenizer),\n",
    "\tcompute_metrics = compute_metrics,\n",
    ")\n"
   ],
   "id": "92839e841ad89c62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vtsoumpris\\AppData\\Local\\Temp\\ipykernel_5236\\902408632.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Start fine-tuning",
   "id": "57b08c612fd471de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T22:01:02.582054Z",
     "start_time": "2025-01-21T21:44:58.887416Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "e3182054a5155275",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/300 07:30 < 37:08:35, 0.00 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:2171\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2169\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2172\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2176\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:2531\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2524\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2525\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2527\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2528\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2529\u001B[0m )\n\u001B[0;32m   2530\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2531\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2534\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2535\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2536\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2537\u001B[0m ):\n\u001B[0;32m   2538\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2539\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:3678\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   3676\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs)\n\u001B[0;32m   3677\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3678\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3680\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[0;32m   3681\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   3682\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   3683\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   3684\u001B[0m ):\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:3734\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   3732\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[0;32m   3733\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n\u001B[1;32m-> 3734\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3735\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[0;32m   3736\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[0;32m   3737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:1103\u001B[0m, in \u001B[0;36mDebertaForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1095\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1096\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1101\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1103\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1104\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1106\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1108\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1109\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1110\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1112\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1114\u001B[0m encoder_layer \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1115\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(encoder_layer)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:797\u001B[0m, in \u001B[0;36mDebertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    787\u001B[0m     token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m    789\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    790\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    791\u001B[0m     token_type_ids\u001B[38;5;241m=\u001B[39mtoken_type_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    794\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[0;32m    795\u001B[0m )\n\u001B[1;32m--> 797\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m encoded_layers \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    806\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mz_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:608\u001B[0m, in \u001B[0;36mDebertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001B[0m\n\u001B[0;32m    598\u001B[0m     hidden_states, att_m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    599\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    600\u001B[0m         next_kv,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    605\u001B[0m         output_attentions,\n\u001B[0;32m    606\u001B[0m     )\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 608\u001B[0m     hidden_states, att_m \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnext_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    610\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrelative_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrelative_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrel_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrel_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    617\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[0;32m    618\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_states,)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:525\u001B[0m, in \u001B[0;36mDebertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001B[0m\n\u001B[0;32m    516\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    518\u001B[0m     hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    523\u001B[0m     output_attentions: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    524\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, Optional[torch\u001B[38;5;241m.\u001B[39mTensor]]:\n\u001B[1;32m--> 525\u001B[0m     attention_output, att_matrix \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrelative_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrelative_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrel_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrel_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    533\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate(attention_output)\n\u001B[0;32m    534\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:460\u001B[0m, in \u001B[0;36mDebertaAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001B[0m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    452\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    453\u001B[0m     hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    458\u001B[0m     rel_embeddings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    459\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, Optional[torch\u001B[38;5;241m.\u001B[39mTensor]]:\n\u001B[1;32m--> 460\u001B[0m     self_output, att_matrix \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    461\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrelative_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrelative_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrel_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrel_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    469\u001B[0m         query_states \u001B[38;5;241m=\u001B[39m hidden_states\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:298\u001B[0m, in \u001B[0;36mDisentangledSelfAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001B[0m\n\u001B[0;32m    295\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_weights_proj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    296\u001B[0m     attention_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_weights_proj(attention_probs\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m--> 298\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_probs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue_layer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    299\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m context_layer\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    300\u001B[0m new_context_layer_shape \u001B[38;5;241m=\u001B[39m context_layer\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Validate fine-tuned model",
   "id": "316e5a584b9b71a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T22:01:04.131652800Z",
     "start_time": "2025-01-20T18:12:25.652363Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.evaluate()",
   "id": "39e8497ae9c42335",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6801918148994446,\n",
       " 'eval_accuracy': 0.725,\n",
       " 'eval_precision': 0.7102803738317757,\n",
       " 'eval_recall': 0.76,\n",
       " 'eval_f1': 0.7342995169082126,\n",
       " 'eval_runtime': 51.9587,\n",
       " 'eval_samples_per_second': 3.849,\n",
       " 'eval_steps_per_second': 0.962,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T22:01:04.809783100Z",
     "start_time": "2025-01-20T18:13:17.732350Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model.save_pretrained(\"./vtsoumpris/fnc-deberta-lora\")",
   "id": "d2900e98d50d4751",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T22:01:05.093782300Z",
     "start_time": "2025-01-20T18:13:55.126737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make a dataframe with the predictions and the text and the labels\n",
    "items_for_manual_review = tokenized_dataset[\"test\"].select(\n",
    "    [0, 1, 22, 31, 43, 199, 150, 40]\n",
    ")\n",
    "\n",
    "results = trainer.predict(items_for_manual_review)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"complaint\": [item[\"complaint\"] for item in items_for_manual_review],\n",
    "        \"predictions\": results.predictions.argmax(axis=1),\n",
    "        \"labels\": results.label_ids,\n",
    "    }\n",
    ")\n",
    "# Show all the cell\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df"
   ],
   "id": "909478a8b3b51301",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       complaint  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              I reported fraudulent activity on my visa credit card with Bank America. There were two fraudulent charges made on XX/XX/XXXX and XX/XX/XXXX at the same location in amount of {$39.00}. I filed a claim with Bank of America and they denied the claim with no explanation other than saying I have too many accounts with this pizza place, which makes no sense. I contacted them again and they said they'd have to file an appeal it would be another 45-90 days. In the meantime while I'm waiting for appeal, they added the two charges back into my account which is continue to accrue interest. Please help me with these fraudulent charges and unfair treatment from Bank of America   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I had decided to cancel my homeowners insurance with one company, and go with another company 2 months prior to the end date of the policy. I had gone to a local branch to inform of the change, and make sure the company that I was leaving would not be paid and the new one would be paid for the upcoming year renewal. The representative took down all the information, and stated that she would make sure the now old insurance company would n't be paid, and would only release a payment to the new insurance company. I received a notice this week to inform that both insurance companies have been paid in a total amount over {$2400.00}. After paying two different insurance company Bank of America ran an out-of-cycle escrow analysis report knowing that two insurance companies had been paid ; which will cause my mortgage payment to increase effective XX/XX/XXXX, 2018 close to {$200.00}. I have contacted the branch representative who stated that she had forgot to get the information over to the mortgage company, and it was my responsibility to contact the insurance company to get the refund back on a cancelled policy. I was informed by the insurance company that a refund could take up to 7 weeks. In the meantime, the mortgage company knowing that an error was made on there end will not remove the escrow analysis which will cause the mortgage payment to increase.   \n",
       "2                                                                                                                                                                                                                                                     There are already multiple complaints that Bank of America ignored. Their regulatory complaint staff refuse to return phone calls. On XXXX XXXX, XXXX, I got scammed and thought I was speaking to Bank of America corporate. Unfortunately, it was a criminal enterprise unknown to me when I was trying to verify my information with the crook to get help. As the fraud was happening, I contacted Bank of America right when the fraud was occurring ( the thief without my knowledge or consent obtained cash advances on 4 of my credit cards and somehow sent the money out on one of my checking accounts. ) The fraud dept sadistically insisted that as a woman age XXXX and high risk for covid go into a branch and refused to speak to me. No one was allowed to talk to me. No fraud forms were mailed to me asking for an explanation and a signature which proves they weren't doing anything. Finally, I lucked out with the TN call center and the total freeze was removed. I was able to close my demand deposit accounts. Previous complaints were ignored and their regulatory complaints rep refuses to call me back. Due to the compassion of one fraud person who made an appointment, I did go into the XXXX XXXX and XXXX XXXX was masked and we were in his office. ( A very nasty and sadistic creature named XXXX at the XXXX fraud center stooped so low she took away my online banking when I called when I saw a {$2000.00} fraud credit card balance. ) At that time, only my card ending in XXXX hat was reissued and Iused, had a legitimate balance of {$200.00} ) XXXX showed me that the other credit cards had 0 balances. I sat in his office for an hour while he was on hold with the fraud dept, I left and he continued to hold for them for another hour. XXXX called me -his number is XXXX and advised me that the fraud claims were settled per his conversation ). I believe XXXX but he was given incorrect information. Sadly, the following XXXX claims are denied on XX/XX/XXXX and XX/XX/XXXX : Account ending in XXXX Claim # XXXX amount {$2000.00} Account ending in XXXX Claim # XXXX amount {$1800.00} When the nightmare fraud was reported on XX/XX/XXXX, I was given reference # XXXX XXXX XXXX XXXX XXXX. Again, those fraudulent cash advances- which the bank fraud dept should have realized was not within any pattern I ever made. I never made a cash advance in my life!!!!!! It would have helped if they would have spoken to me. \\n\\nI am attaching a copy of my police report and even tho I had given the initial person XXXX the police report number, I was told on XX/XX/XXXX that it would help. Apparently, the bank was too inept to even ask for it or try to obtain it. I am attaching the police report. The investigator had to obtain a subpoena and I had called me to say the funds were transferred somehow to an entiry or person in Texas so she has no jurisdiction. I also contend that the fraud dept should have and could have stopped those transfers from going out. I am also attaching screen shots that show the bank knew there was fraud from the start and failed to do due dilligence. \\n\\nAnd ... ..apparently refusing or neglecting to investigate fraud is standard operating procedure with Bank of America and it is a widespread practice. I am attaching the civil complaint : XXXX XXXX XXXX on behalf of herself and all Plaintiff, Civil No. XXXX BANK OF AMERICA, N.A. , ) Defendant. \\nCLASS ACTION COMPLAINT Apparently,, I am not the only one whose fraud credit card claims are being ignored and they are holding the individual responsible - even tho the fraud was reported. \\n\\nI do not owe Bank of America the money and they totally neglected to investigate those fraud claims on 2 of my credit cards. I am also attaching the most recent screen shot showing those 2 balances. Who knows what else will show up.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The fraudulent billings on my Bank of America Mastercard XXXX came to my attention on XXXX XX/XX/XXXX after review of the prior year 's Bank of America charge card statements for XXXX. The review was done to gather information to prepare my federal tax return for XXXX. \\nThese fraudulent billings to this credit card all had one thing in common - XXXX ( merchants name ) '. After researching these items with a call to XXXX on XX/XX/XXXX, it became clear they were billed from a rogue account. This account had been opened by a relative of one of my employees. This employee had been hired a few years earlier to help me manage my sole proprietor business. I provide eye care services and products. \\n\\nI called Bank of America on XX/XX/XXXX, with the list of fraudulent billings from this rogue account per XXXX fraud department. Bank of America 's fraud department closed the account, and issued a new credit card XXXX. They investigated the matter. They agreed, and found all the fraudulent billings. They issued credits of {$68000.00}. I agreed this was correct, and they closed the case - per letter from Bank of America XX/XX/XXXX. \\n\\nI had also filed a report with the XXXX Police Department on XX/XX/XXXX. The report # XXXX was investigated by Detective XXXX. He did confirm the XXXX account is fraudulent after issuing subpoenas. The fraud case is ongoing against those involved as other issues arose from this investigation. All this information had been given to Bank of America. \\n\\nSurprisingly, on XX/XX/XXXX, I received a letter from Bank of America charging back /denying all my credits. This letter stated the merchant ( fraudster ) reported I received product and had a subscription that was delivered to my address. Again, I called Bank of America to complain the fraudulent billings were unauthorized. They had been paid by my accounting staff at the time ( rogue employee ) with billings from her relative 's rogue XXXX account. \\n\\nI wrote a letter on XXXX XXXX to Bank of America in reply. However, they denied my request. Thus, I closed the Bank of America account ending XXXX. I have not paid any of the fraud charges to date. Thus, my credit score was ruined since Bank of America reported this. They continue to refuse to issue back unauthorized billing credits. \\n\\nI believe XXXX had a reponsibility in detecting fraudulent activity, and the opening of this rogue account. If their systems detected numerous billings going out of this rogue account to just my Bank of America account -- it could have been prevented. Bank of America should have contacted Bank of America to confirm this account was indeed fraudulent -- as the XXXX Police Department found through subpoenas. \\n\\nI also believe Bank of America should adhere to their written policy in not having consumers be responsible to pay for fraudulent billings. If they investigated the matter thoroughly by contacting XXXX, this matter would have been resolved correclty. \\n\\nIn summary, I have exhausted all other avenues to have my Bank of America account credited back appropriately. I now hope to have CFPB to investigate this matter to apply back credits due to fraud.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We applied for a home mortgage refinancing with BoA XX/XX/XXXX. After we locked in a market rate, they continually reappraised our home. They continually rejected appraisals until they received a low appraisal and then charged us {$6300.00} of closing cost points because we had become a \" high-risk '' asset mortgage. \\nThe appraisal they eventually accepted was XXXX - XXXX % below the other appraisals that they rejected until they received one appraisal that forced us to pay points. Since interest rates increased over loan processing time, we had to pay the {$6300.00} of points. \\nBoA said they have no influence over appraisal selection due to Dodd Frank, but they kept rejecting \" independent '' appraisals until they found one low enough to force us to pay points. \\nI spoke to the appraisers whose appraisals were rejected and they told me the appraisals were over XXXX % higher than the one low-ball that BoA eventually accepted. From BoA here was no explanation of the rejections, only constant streams of appraisers flowing through our house until they finally found an appraisal XXXX % below market that forced us to pay points. I tried to speak with BoA management. They never called to explain the multiple rejected appraisals or the assignment of points.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I was in process of completing a loan for the purchase of my first home with XXXX. I went into where I had my current checking account at Bank if America here in XXXX XXXX. When I went to send the wire the banker stated they would not charge me for the wire if I spoke with their mortgage banker to review their offer. I then sat with XXXX XXXX XXXX XXXX. He then described how if I wrote an email to him stating I had a certain rate and points from XXXX he would be able to match that quote. He then proceeded to write the verbiage I needed to say. He told me to copy paste what he wrote and sign it for him. The rate he gave me initially was at a 1 point cost. He said with this letter he could waive that point cost. I later found out that due to the type of loan program and amount I was putting down that Bank of America didnt even truly offer that product for my situation. I felt very uncomfortable, almost like I was committing fraud. I called my mortgage banker and explained how I felt. I realized I wanted to write this report to explain how I felt and did my own research realizing that the process of XXXX writing the letter for me and using an letter to quote match is not actually the correct process to complete a quote match.   \n",
       "6  I began receiving the following documents from Bank of America ( BofA ) and XXXX XXXX, XXXX ( Atty ) : 1. XXXX/XXXX/2016 : US Bankruptcy Court Statement In Response To Notice Of Final Cure Payment ( Filed XXXX/XXXX/2016 ) stating Pre-Petition Default Payments that BofA, N.A., \" Agrees that Debtor ( s ) has paid in full the amount required to cure the default on Creditor 's claim '' and Post-Petition Default Payments that BofA, N.A., \" Disagrees that Debtor ( s ) is current with respect to all payments consistent with 1322 ( b ) ( 5 ), and states that the total amount due to cure post petition arrears is : Total Amount Due : {$8200.00} '' ( Atty ) XXXX. XXXX/XXXX/2016 : NOTICE OF THE RIGHT TO CURE THE DEFAULT AND INTENT TO ACCELERATE ( dtd XXXX 2016 ) stating, \" The home loan is in serious default because the required payments have not been made. Bank of America , N.A . has the right to begin the process of foreclosing on the debt and may initiate foreclosure at any time after forty ( 40 ) days from the date of this notice ... '' ( BofA ) XXXX. XXXX/XXXX/2016 : Bank of America Home Loans Statement ( dtd XXXX/XXXX/2016 ) indicating the Total amount due is {$9200.00}. \\nXXXX. XXXX/XXXX/2016 : Bank of America Home Loans Borrower Response Package ( dtd XXXX/XXXX/2016 ) stating, \" Our records indicate you have not made your last four or more regularly scheduled payments. Subject to applicable law, foreclosure activities typically begin after four missed payments, so it is important that you take action on this issue quickly. Ignoring the situation and continuing to let your payments become past due will put you at risk of losing your home to foreclosure ... '' XXXX. XXXX/XXXX/2016 : NOTICE OF FORECLOSURE SALE ( dtd XXXX/XXXX/2016 ) stating, \" By letter dated XXXX XXXX, 2016 ( the \" Initial Communication Letter '' ) we notified you that the above-referenced loan had been referred to this law firm for handling ... '' ( Atty ) XXXX. XXXX/XXXX/2016 : \" EXCEPT AS MAY BE NOTED HEREIN, THIS IS AN ATTEMPT TO COLLECT A DEBT. ANY INFORMATION OBTAINED WILL BE USED FOR THAT PURPOSE. '' ( dtd XXXX/XXXX/2016 ) ( Atty ) XXXX. XXXX/XXXX/2016 : Bank of America Home Loans ( dtd XXXX/XXXX/2016 ) stating, \" Based on a careful review of your loan, we are offering you an opportunity to enter into a Trial Period for a loan modification ... '' I have in fact made payments for the entire time period that BofA and XXXX XXXX alleges that I have not made in all of the overwhelming notices that I have received thus far. For over three weeks I spoke to numerous Bank of America personnel in various departments ( See communication notes ) who transferred me back and forth between them with no resolution, and would not accept my XXXX 2016 payment. From the customer view, it appears that the left hand is not aligned with the right hand and that the head has been cut off! \\nAlso, on XXXX/XXXX/2016, I requested my payment history from BofA, including the time they allege I missed payments. The transaction details clearly show where they have received my payments during this same timeframe. There were a lot of unexplained reversals that I have been awaiting an explanation for from BofA 's Ledger and Balance Department since XXXX/XXXX/2016. \\nIn addition, I responded to all of the notices of allegations ; hand delivered to XXXX XXXX, XXXX ( XXXX/XXXX/2016 ) ; and mailed via US Postal Service certified return receipt to BofA Home Loans in XXXX, XXXX and XXXX, XXXX ( XXXX/XXXX/2016 ) ( See attached ) Lastly, this distressing ordeal, dealing with BofA and XXXX XXXX have interfered with my ability to obtain a suitable consumer credit for a car loan. My life is on hold and I have been in \" WAIT MODE '' since I started receiving these defamatory allegations. I have acted in good faith and BofA and XXXX XXXX have failed to conduct due diligence, appeared to have failed in meeting their legal obligations, and are operating with broken business practices. \\nThis entire ordeal has caused me extreme undue stress to no avail!   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          My mortgage is serviced by Bank of America. I have a reoccurring payment set up and have for a number of years. This year I got a letter saying my payment was late and they were going to charge me a fee. \\n\\nI never canceled this autopayment. \\n\\nSo after calling them and nobody could tell me why this happened, they credited me the fee, and I made a manual payment and they said the auto payments will resume. \\n\\nI checked for my XXXX payment, and the auto paymets show \" Canceled '' I called them, and nobody knows why. They keep canceling my auto pay. \\n\\n\\nI believe they are committing fraud by coming up with scenarios to charge fees, hoping we don't find out, like XXXX XXXX has done.   \n",
       "\n",
       "   predictions  labels  \n",
       "0            1       1  \n",
       "1            0       0  \n",
       "2            1       1  \n",
       "3            0       1  \n",
       "4            0       0  \n",
       "5            1       0  \n",
       "6            0       0  \n",
       "7            1       0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I reported fraudulent activity on my visa credit card with Bank America. There were two fraudulent charges made on XX/XX/XXXX and XX/XX/XXXX at the same location in amount of {$39.00}. I filed a claim with Bank of America and they denied the claim with no explanation other than saying I have too many accounts with this pizza place, which makes no sense. I contacted them again and they said they'd have to file an appeal it would be another 45-90 days. In the meantime while I'm waiting for appeal, they added the two charges back into my account which is continue to accrue interest. Please help me with these fraudulent charges and unfair treatment from Bank of America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had decided to cancel my homeowners insurance with one company, and go with another company 2 months prior to the end date of the policy. I had gone to a local branch to inform of the change, and make sure the company that I was leaving would not be paid and the new one would be paid for the upcoming year renewal. The representative took down all the information, and stated that she would make sure the now old insurance company would n't be paid, and would only release a payment to the new insurance company. I received a notice this week to inform that both insurance companies have been paid in a total amount over {$2400.00}. After paying two different insurance company Bank of America ran an out-of-cycle escrow analysis report knowing that two insurance companies had been paid ; which will cause my mortgage payment to increase effective XX/XX/XXXX, 2018 close to {$200.00}. I have contacted the branch representative who stated that she had forgot to get the information over to the mortgage company, and it was my responsibility to contact the insurance company to get the refund back on a cancelled policy. I was informed by the insurance company that a refund could take up to 7 weeks. In the meantime, the mortgage company knowing that an error was made on there end will not remove the escrow analysis which will cause the mortgage payment to increase.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are already multiple complaints that Bank of America ignored. Their regulatory complaint staff refuse to return phone calls. On XXXX XXXX, XXXX, I got scammed and thought I was speaking to Bank of America corporate. Unfortunately, it was a criminal enterprise unknown to me when I was trying to verify my information with the crook to get help. As the fraud was happening, I contacted Bank of America right when the fraud was occurring ( the thief without my knowledge or consent obtained cash advances on 4 of my credit cards and somehow sent the money out on one of my checking accounts. ) The fraud dept sadistically insisted that as a woman age XXXX and high risk for covid go into a branch and refused to speak to me. No one was allowed to talk to me. No fraud forms were mailed to me asking for an explanation and a signature which proves they weren't doing anything. Finally, I lucked out with the TN call center and the total freeze was removed. I was able to close my demand deposit accounts. Previous complaints were ignored and their regulatory complaints rep refuses to call me back. Due to the compassion of one fraud person who made an appointment, I did go into the XXXX XXXX and XXXX XXXX was masked and we were in his office. ( A very nasty and sadistic creature named XXXX at the XXXX fraud center stooped so low she took away my online banking when I called when I saw a {$2000.00} fraud credit card balance. ) At that time, only my card ending in XXXX hat was reissued and Iused, had a legitimate balance of {$200.00} ) XXXX showed me that the other credit cards had 0 balances. I sat in his office for an hour while he was on hold with the fraud dept, I left and he continued to hold for them for another hour. XXXX called me -his number is XXXX and advised me that the fraud claims were settled per his conversation ). I believe XXXX but he was given incorrect information. Sadly, the following XXXX claims are denied on XX/XX/XXXX and XX/XX/XXXX : Account ending in XXXX Claim # XXXX amount {$2000.00} Account ending in XXXX Claim # XXXX amount {$1800.00} When the nightmare fraud was reported on XX/XX/XXXX, I was given reference # XXXX XXXX XXXX XXXX XXXX. Again, those fraudulent cash advances- which the bank fraud dept should have realized was not within any pattern I ever made. I never made a cash advance in my life!!!!!! It would have helped if they would have spoken to me. \\n\\nI am attaching a copy of my police report and even tho I had given the initial person XXXX the police report number, I was told on XX/XX/XXXX that it would help. Apparently, the bank was too inept to even ask for it or try to obtain it. I am attaching the police report. The investigator had to obtain a subpoena and I had called me to say the funds were transferred somehow to an entiry or person in Texas so she has no jurisdiction. I also contend that the fraud dept should have and could have stopped those transfers from going out. I am also attaching screen shots that show the bank knew there was fraud from the start and failed to do due dilligence. \\n\\nAnd ... ..apparently refusing or neglecting to investigate fraud is standard operating procedure with Bank of America and it is a widespread practice. I am attaching the civil complaint : XXXX XXXX XXXX on behalf of herself and all Plaintiff, Civil No. XXXX BANK OF AMERICA, N.A. , ) Defendant. \\nCLASS ACTION COMPLAINT Apparently,, I am not the only one whose fraud credit card claims are being ignored and they are holding the individual responsible - even tho the fraud was reported. \\n\\nI do not owe Bank of America the money and they totally neglected to investigate those fraud claims on 2 of my credit cards. I am also attaching the most recent screen shot showing those 2 balances. Who knows what else will show up.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The fraudulent billings on my Bank of America Mastercard XXXX came to my attention on XXXX XX/XX/XXXX after review of the prior year 's Bank of America charge card statements for XXXX. The review was done to gather information to prepare my federal tax return for XXXX. \\nThese fraudulent billings to this credit card all had one thing in common - XXXX ( merchants name ) '. After researching these items with a call to XXXX on XX/XX/XXXX, it became clear they were billed from a rogue account. This account had been opened by a relative of one of my employees. This employee had been hired a few years earlier to help me manage my sole proprietor business. I provide eye care services and products. \\n\\nI called Bank of America on XX/XX/XXXX, with the list of fraudulent billings from this rogue account per XXXX fraud department. Bank of America 's fraud department closed the account, and issued a new credit card XXXX. They investigated the matter. They agreed, and found all the fraudulent billings. They issued credits of {$68000.00}. I agreed this was correct, and they closed the case - per letter from Bank of America XX/XX/XXXX. \\n\\nI had also filed a report with the XXXX Police Department on XX/XX/XXXX. The report # XXXX was investigated by Detective XXXX. He did confirm the XXXX account is fraudulent after issuing subpoenas. The fraud case is ongoing against those involved as other issues arose from this investigation. All this information had been given to Bank of America. \\n\\nSurprisingly, on XX/XX/XXXX, I received a letter from Bank of America charging back /denying all my credits. This letter stated the merchant ( fraudster ) reported I received product and had a subscription that was delivered to my address. Again, I called Bank of America to complain the fraudulent billings were unauthorized. They had been paid by my accounting staff at the time ( rogue employee ) with billings from her relative 's rogue XXXX account. \\n\\nI wrote a letter on XXXX XXXX to Bank of America in reply. However, they denied my request. Thus, I closed the Bank of America account ending XXXX. I have not paid any of the fraud charges to date. Thus, my credit score was ruined since Bank of America reported this. They continue to refuse to issue back unauthorized billing credits. \\n\\nI believe XXXX had a reponsibility in detecting fraudulent activity, and the opening of this rogue account. If their systems detected numerous billings going out of this rogue account to just my Bank of America account -- it could have been prevented. Bank of America should have contacted Bank of America to confirm this account was indeed fraudulent -- as the XXXX Police Department found through subpoenas. \\n\\nI also believe Bank of America should adhere to their written policy in not having consumers be responsible to pay for fraudulent billings. If they investigated the matter thoroughly by contacting XXXX, this matter would have been resolved correclty. \\n\\nIn summary, I have exhausted all other avenues to have my Bank of America account credited back appropriately. I now hope to have CFPB to investigate this matter to apply back credits due to fraud.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We applied for a home mortgage refinancing with BoA XX/XX/XXXX. After we locked in a market rate, they continually reappraised our home. They continually rejected appraisals until they received a low appraisal and then charged us {$6300.00} of closing cost points because we had become a \" high-risk '' asset mortgage. \\nThe appraisal they eventually accepted was XXXX - XXXX % below the other appraisals that they rejected until they received one appraisal that forced us to pay points. Since interest rates increased over loan processing time, we had to pay the {$6300.00} of points. \\nBoA said they have no influence over appraisal selection due to Dodd Frank, but they kept rejecting \" independent '' appraisals until they found one low enough to force us to pay points. \\nI spoke to the appraisers whose appraisals were rejected and they told me the appraisals were over XXXX % higher than the one low-ball that BoA eventually accepted. From BoA here was no explanation of the rejections, only constant streams of appraisers flowing through our house until they finally found an appraisal XXXX % below market that forced us to pay points. I tried to speak with BoA management. They never called to explain the multiple rejected appraisals or the assignment of points.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I was in process of completing a loan for the purchase of my first home with XXXX. I went into where I had my current checking account at Bank if America here in XXXX XXXX. When I went to send the wire the banker stated they would not charge me for the wire if I spoke with their mortgage banker to review their offer. I then sat with XXXX XXXX XXXX XXXX. He then described how if I wrote an email to him stating I had a certain rate and points from XXXX he would be able to match that quote. He then proceeded to write the verbiage I needed to say. He told me to copy paste what he wrote and sign it for him. The rate he gave me initially was at a 1 point cost. He said with this letter he could waive that point cost. I later found out that due to the type of loan program and amount I was putting down that Bank of America didnt even truly offer that product for my situation. I felt very uncomfortable, almost like I was committing fraud. I called my mortgage banker and explained how I felt. I realized I wanted to write this report to explain how I felt and did my own research realizing that the process of XXXX writing the letter for me and using an letter to quote match is not actually the correct process to complete a quote match.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I began receiving the following documents from Bank of America ( BofA ) and XXXX XXXX, XXXX ( Atty ) : 1. XXXX/XXXX/2016 : US Bankruptcy Court Statement In Response To Notice Of Final Cure Payment ( Filed XXXX/XXXX/2016 ) stating Pre-Petition Default Payments that BofA, N.A., \" Agrees that Debtor ( s ) has paid in full the amount required to cure the default on Creditor 's claim '' and Post-Petition Default Payments that BofA, N.A., \" Disagrees that Debtor ( s ) is current with respect to all payments consistent with 1322 ( b ) ( 5 ), and states that the total amount due to cure post petition arrears is : Total Amount Due : {$8200.00} '' ( Atty ) XXXX. XXXX/XXXX/2016 : NOTICE OF THE RIGHT TO CURE THE DEFAULT AND INTENT TO ACCELERATE ( dtd XXXX 2016 ) stating, \" The home loan is in serious default because the required payments have not been made. Bank of America , N.A . has the right to begin the process of foreclosing on the debt and may initiate foreclosure at any time after forty ( 40 ) days from the date of this notice ... '' ( BofA ) XXXX. XXXX/XXXX/2016 : Bank of America Home Loans Statement ( dtd XXXX/XXXX/2016 ) indicating the Total amount due is {$9200.00}. \\nXXXX. XXXX/XXXX/2016 : Bank of America Home Loans Borrower Response Package ( dtd XXXX/XXXX/2016 ) stating, \" Our records indicate you have not made your last four or more regularly scheduled payments. Subject to applicable law, foreclosure activities typically begin after four missed payments, so it is important that you take action on this issue quickly. Ignoring the situation and continuing to let your payments become past due will put you at risk of losing your home to foreclosure ... '' XXXX. XXXX/XXXX/2016 : NOTICE OF FORECLOSURE SALE ( dtd XXXX/XXXX/2016 ) stating, \" By letter dated XXXX XXXX, 2016 ( the \" Initial Communication Letter '' ) we notified you that the above-referenced loan had been referred to this law firm for handling ... '' ( Atty ) XXXX. XXXX/XXXX/2016 : \" EXCEPT AS MAY BE NOTED HEREIN, THIS IS AN ATTEMPT TO COLLECT A DEBT. ANY INFORMATION OBTAINED WILL BE USED FOR THAT PURPOSE. '' ( dtd XXXX/XXXX/2016 ) ( Atty ) XXXX. XXXX/XXXX/2016 : Bank of America Home Loans ( dtd XXXX/XXXX/2016 ) stating, \" Based on a careful review of your loan, we are offering you an opportunity to enter into a Trial Period for a loan modification ... '' I have in fact made payments for the entire time period that BofA and XXXX XXXX alleges that I have not made in all of the overwhelming notices that I have received thus far. For over three weeks I spoke to numerous Bank of America personnel in various departments ( See communication notes ) who transferred me back and forth between them with no resolution, and would not accept my XXXX 2016 payment. From the customer view, it appears that the left hand is not aligned with the right hand and that the head has been cut off! \\nAlso, on XXXX/XXXX/2016, I requested my payment history from BofA, including the time they allege I missed payments. The transaction details clearly show where they have received my payments during this same timeframe. There were a lot of unexplained reversals that I have been awaiting an explanation for from BofA 's Ledger and Balance Department since XXXX/XXXX/2016. \\nIn addition, I responded to all of the notices of allegations ; hand delivered to XXXX XXXX, XXXX ( XXXX/XXXX/2016 ) ; and mailed via US Postal Service certified return receipt to BofA Home Loans in XXXX, XXXX and XXXX, XXXX ( XXXX/XXXX/2016 ) ( See attached ) Lastly, this distressing ordeal, dealing with BofA and XXXX XXXX have interfered with my ability to obtain a suitable consumer credit for a car loan. My life is on hold and I have been in \" WAIT MODE '' since I started receiving these defamatory allegations. I have acted in good faith and BofA and XXXX XXXX have failed to conduct due diligence, appeared to have failed in meeting their legal obligations, and are operating with broken business practices. \\nThis entire ordeal has caused me extreme undue stress to no avail!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My mortgage is serviced by Bank of America. I have a reoccurring payment set up and have for a number of years. This year I got a letter saying my payment was late and they were going to charge me a fee. \\n\\nI never canceled this autopayment. \\n\\nSo after calling them and nobody could tell me why this happened, they credited me the fee, and I made a manual payment and they said the auto payments will resume. \\n\\nI checked for my XXXX payment, and the auto paymets show \" Canceled '' I called them, and nobody knows why. They keep canceling my auto pay. \\n\\n\\nI believe they are committing fraud by coming up with scenarios to charge fees, hoping we don't find out, like XXXX XXXX has done.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a1eb7a39b72292f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

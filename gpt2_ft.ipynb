{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine Tuning GPT-2",
   "id": "ab1b51ff62410a34"
  },
  {
   "cell_type": "code",
   "id": "3afc9c71868e81e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:29:09.081968Z",
     "start_time": "2025-01-19T15:29:09.055121Z"
    }
   },
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "mdl_tok_name = \"gpt2\""
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Lora Configuration for GPT-2 model",
   "id": "c097bfd4ea37c7b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:29:09.143817Z",
     "start_time": "2025-01-19T15:29:09.134168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config = \\\n",
    "LoraConfig(\n",
    "\tr = 16,  # Low-rank dimension: Start with 16, modify based on model size\n",
    "\ttarget_modules = [\"c_attn\", \"c_proj\"],  # Correct target modules for GPT-2\n",
    "\ttask_type = TaskType.SEQ_CLS,  # Task type, e.g., Sequence Classification\n",
    "\tlora_alpha = 32,  # Scaling factor, consider increasing for larger models\n",
    "\tlora_dropout = 0.05  # Dropout, increase slightly if facing overfitting\n",
    ")"
   ],
   "id": "7ab2a33cef283a6",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading the filtered dataset",
   "id": "d044bb1571cbc875"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-19T15:43:28.437281Z",
     "start_time": "2025-01-19T15:43:27.921835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the file path to the dataset\n",
    "file_path = Path(\"data/filtered_dataset.csv\")\n",
    "\n",
    "# Load the dataset using Hugging Face's `load_dataset`\n",
    "dataset = load_dataset('csv', data_files = str(file_path))\n",
    "\n",
    "# Inspect the unique values in the 'labels' column\n",
    "product_classes = dataset[\"train\"].unique(\"Product\")\n",
    "\n",
    "# Convert the 'Product' column to a ClassLabel feature\n",
    "product_label = ClassLabel(names=product_classes)\n",
    "dataset = dataset.cast_column(\"Product\", product_label)\n",
    "\n",
    "# Rename the columns: \"Product\" to \"labels\", and \"Consumer complaint narrative\" to \"complaint\"\n",
    "dataset = dataset.rename_column(\"Product\", \"labels\")\n",
    "dataset = dataset.rename_column(\"Consumer complaint narrative\", \"complaint\")\n",
    "\n",
    "# Extract the features (columns) we want\n",
    "dataset = \\\n",
    "    dataset[\"train\"].select_columns(\n",
    "        [\"complaint\", \"labels\"]\n",
    "    ).train_test_split(\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        seed=23,\n",
    "        stratify_by_column=\"labels\"\n",
    "    )\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the resulting dataset\n",
    "print(dataset)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['complaint', 'labels'],\n",
      "        num_rows: 5312\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['complaint', 'labels'],\n",
      "        num_rows: 1328\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Inspecting the labels\n",
    "\n",
    "Credit card is labeled as 0 and  Mortgage is labeled as 1"
   ],
   "id": "5515a2b1ef507acb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:43:33.944411Z",
     "start_time": "2025-01-19T15:43:33.936761Z"
    }
   },
   "cell_type": "code",
   "source": "product_label",
   "id": "b9040be5e710a56f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['Credit card', 'Mortgage'], id=None)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Preprocess dataset\n",
    "\n",
    "Tokenizing 'Consumer complaint narrative' feature values"
   ],
   "id": "d33a3db87bf72169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:44:27.781489Z",
     "start_time": "2025-01-19T15:44:23.285402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(mdl_tok_name)\n",
    "\n",
    "# Check if the tokenizer already has a pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "\t# Option 1: Use an existing token as the pad_token\n",
    "\ttokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token if suitable\n",
    "\n",
    "\t# Option 2: Add a new padding token if no suitable token exists\n",
    "\t#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\t#tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "# Let's use a lambda function to tokenize all the examples\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"complaint\"], truncation=True,\n",
    "                            padding=True, #\"max_length\"\n",
    "                            return_tensors = \"pt\"\n",
    "                            ),\n",
    "\t    batched=True,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ],
   "id": "4de95d07c818cebd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5312/5312 [00:03<00:00, 1610.18 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1328/1328 [00:00<00:00, 1633.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['complaint', 'labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5312\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:44:33.743633Z",
     "start_time": "2025-01-19T15:44:33.740119Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "11803dd170c0c51e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading Model",
   "id": "7edce09ded4bded4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:44:45.590699Z",
     "start_time": "2025-01-19T15:44:43.766518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    mdl_tok_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"Credit card\", 1: \"Mortgage\"},\n",
    "    label2id={\"Credit card\": 0, \"Mortgage\": 1},\n",
    ")\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token for GPT2\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(model)"
   ],
   "id": "382a8fe8d3b4e8d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:44:46.906603Z",
     "start_time": "2025-01-19T15:44:46.889019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you added new tokens, resize the model's embeddings accordingly\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "id": "a25fe247758321e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:44:48.692885Z",
     "start_time": "2025-01-19T15:44:48.590043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "id": "92bbb4de048aa7a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,623,552 || all params: 126,064,896 || trainable%: 1.2879\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining Evaluation Metrics as a function",
   "id": "3c751fe02b91ed22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:44:59.251457Z",
     "start_time": "2025-01-19T15:44:59.245844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics_v1(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ],
   "id": "9d9f2ed9e41fd752",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:45:00.814386Z",
     "start_time": "2025-01-19T15:45:00.808684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\t# Unpack predictions and labels\n",
    "\tpredictions, labels = eval_pred\n",
    "\t# Get the predicted class (argmax selects the class with the highest score)\n",
    "\tpredictions = np.argmax(predictions, axis = 1)\n",
    "\t# Compute metrics\n",
    "\taccuracy = accuracy_score(labels, predictions)\n",
    "\tprecision = precision_score(labels, predictions, average = \"binary\")\n",
    "\trecall = recall_score(labels, predictions, average = \"binary\")\n",
    "\tf1 = f1_score(labels, predictions, average = \"binary\")\n",
    "\n",
    "\t# Return all metrics\n",
    "\treturn {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ],
   "id": "a78ec1a63874f292",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Define Trainer to fine-tuning the foundation model\n",
    "\n",
    "The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "\n",
    "You can find more at this [link](https://huggingface.co/docs/transformers/main_classes/trainer)."
   ],
   "id": "fb730de6ffb08b74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:45:09.293298Z",
     "start_time": "2025-01-19T15:45:09.188411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir= \"./data/creditc_mortg\",\n",
    "        # Learning rate\n",
    "        learning_rate= 1e-5,  #2e-5 # Lowered to prevent instability on CPU\n",
    "        # Train/Validate batch size\n",
    "        per_device_train_batch_size= 4,  #16 # Reduce batch size to avoid memory crashes\n",
    "        per_device_eval_batch_size= 4, #16 # Same for evaluation\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy= \"epoch\", # Evaluate at the end of each epoch\n",
    "        save_strategy= \"epoch\", # Save model checkpoint every epoch\n",
    "\t    # Epochs and weight decay\n",
    "        num_train_epochs= 1, # Start with 1 epoch, increase as needed\n",
    "        weight_decay= 0.01,  #Standard weight decay\n",
    "\t    # Resource management\n",
    "\t\tgradient_accumulation_steps= 4,  # Simulate larger batches with accumulation\n",
    "\t    #\n",
    "        load_best_model_at_end= True,\n",
    "\t    no_cuda= True, # Ensure no GPU usage\n",
    "    ),\n",
    "    train_dataset= tokenized_dataset[\"train\"],\n",
    "    eval_dataset= tokenized_dataset[\"test\"],\n",
    "    tokenizer= tokenizer,\n",
    "    data_collator= DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics= compute_metrics,\n",
    ")"
   ],
   "id": "92839e841ad89c62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vtsoumpris\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\vtsoumpris\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\vtsoumpris\\AppData\\Local\\Temp\\ipykernel_26556\\2259607873.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Start fine-tuning",
   "id": "57b08c612fd471de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:33:32.719476Z",
     "start_time": "2025-01-19T15:33:03.143553Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "e3182054a5155275",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[113], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:2171\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2169\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2172\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2176\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:2531\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2524\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2525\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2527\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2528\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2529\u001B[0m )\n\u001B[0;32m   2530\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2531\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2534\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2535\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2536\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2537\u001B[0m ):\n\u001B[0;32m   2538\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2539\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:3678\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   3676\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs)\n\u001B[0;32m   3677\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3678\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3680\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[0;32m   3681\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   3682\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   3683\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   3684\u001B[0m ):\n",
      "File \u001B[1;32m~\\PycharmProjects\\Lightweight_FineTuning_FoundationModel\\venv\\Lib\\site-packages\\transformers\\trainer.py:3755\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   3753\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3754\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m outputs:\n\u001B[1;32m-> 3755\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3756\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3757\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(outputs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. For reference, the inputs it received are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(inputs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3758\u001B[0m         )\n\u001B[0;32m   3759\u001B[0m     \u001B[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001B[39;00m\n\u001B[0;32m   3760\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Validate fine-tuned model",
   "id": "316e5a584b9b71a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "39e8497ae9c42335"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
